# -*- coding: utf-8 -*-
"""Image_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E2F3eTKaM0Kvz4Q6u3glB7WaDvA6a5yB
"""

pip install keras==2.15.0

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt
import random
import warnings
warnings.simplefilter('ignore')

from matplotlib.pyplot import imshow
from keras.preprocessing import image
from keras import applications
import os
import glob
import cv2

import keras
from keras import layers
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Input,Dense,Activation,ZeroPadding2D,BatchNormalization,Flatten,Conv2D
from keras.layers import AveragePooling2D,GlobalAveragePooling2D,GlobalMaxPool2D,MaxPooling2D,MaxPool2D,Dropout
from keras.models import Model,Sequential

from google.colab import files
files.upload()

!mkdir -p ~/.content
!cp Kaggle settings.json ~/.content/
!chmod 600 ~/.content/Kaggle settings.json

!kaggle datasets download -d gpiosenka/sports-classification

!unzip sports-classification.zip

import os
from PIL import Image
import matplotlib.pyplot as plt


# Replace with the actual directory name where the images are unzipped
image_dir = '/content/train/barell racing'
image_files = os.listdir(image_dir)


# Load and display a few images
for image_file in image_files[:1]:  # Display the first 5 images
    image_path = os.path.join(image_dir, image_file)
    # Check if the current file is a directory
    if not os.path.isdir(image_path):
        image = Image.open(image_path)
        plt.imshow(image)
        plt.title(image_file)
        plt.show()

# Replace with the actual directory name where the images are unzipped
image_dir = 'train/sidecar racing'

# List files to confirm the presence of the image
print(os.listdir(image_dir))

# Replace with the actual image file name
image_file = '033.jpg'

# Construct the full path to the image file
image_path = os.path.join(image_dir, image_file)
print(f"Image path: {image_path}")

# Read the image using OpenCV
img = cv2.imread(image_path)

if img is not None:
    print(img.shape)

    # Convert the image from BGR to RGB
    img1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    # Display the image using matplotlib
    plt.imshow(img1)
    plt.title(image_file)
    plt.show()
else:
    print("Image not found or unable to read.")

train_class = os.listdir('train/')
print(train_class)

# List the classes in the training directory
train_dir = 'train/'
train_class = os.listdir(train_dir)
print(train_class)

count_dict1 = {}
img_dict1 = {}

# Loop through classes
for cls in train_class:
    image_path = glob.glob(f'{train_dir}/{cls}/*')
    count_dict1[cls] = len(image_path)

    if image_path:  # Check if image_path is not empty
        img_dict1[cls] = tf.keras.utils.load_img(random.choice(image_path))

# Print the counts
print(count_dict1)

# Determine number of rows and columns for the plot
num_classes = len(train_class)
num_cols = 4
num_rows = (num_classes + num_cols - 1) // num_cols  # Ceiling division

# Set the figure size based on the number of rows
plt.figure(figsize=(num_cols * 4, num_rows * 4))

# Display a random image from each class
for i, (cls, img) in enumerate(img_dict1.items()):
    plt.subplot(num_rows, num_cols, i + 1)
    plt.imshow(img)
    plt.title(cls)
    plt.axis('off')

plt.tight_layout()
plt.show()

# Create a DataFrame from the count dictionary
df1 = pd.DataFrame(data={'label': list(count_dict1.keys()), 'count': list(count_dict1.values())})

df1

# @title count
from matplotlib import pyplot as plt
df1['count'].plot(kind='hist', bins=20, title='count')
plt.gca().spines[['top', 'right',]].set_visible(False)

plt.figure(figsize=(20,8))
sns.barplot(x='label',y='count',data=df1)
plt.xticks(rotation=90)
plt.show()

test_dir=os.listdir('test/')

test_dir

# List the classes in the training directory
test_dir = 'test/'
test_class = os.listdir(train_dir)
print(train_class)

count_dict2 = {}
img_dict2 = {}

# Loop through classes
for cls in test_class:
    image_path = glob.glob(f'{test_dir}/{cls}/*')
    count_dict2[cls] = len(image_path)

    if image_path:  # Check if image_path is not empty
        img_dict2[cls] = tf.keras.utils.load_img(random.choice(image_path))

# Print the counts
print(count_dict2)

# Determine number of rows and columns for the plot
num_classes_test = len(test_class)
num_colss = 4
num_rowss = (num_classes_test + num_colss - 1) // num_colss  # Ceiling division

# Set the figure size based on the number of rows
plt.figure(figsize=(num_colss * 4, num_rowss * 4))

# Display a random image from each class
for i, (cls, img) in enumerate(img_dict2.items()):
    plt.subplot(num_rowss, num_colss, i + 1)
    plt.imshow(img)
    plt.title(cls)
    plt.axis('off')

plt.tight_layout()
plt.show()

# Define ImageDataGenerator for augmentation and normalization
train_datagen = ImageDataGenerator(
    rescale=1./255,  # Normalize pixel values
    rotation_range=40,  # Random rotation
    width_shift_range=0.2,  # Random horizontal shift
    height_shift_range=0.2,  # Random vertical shift
    shear_range=0.2,  # Random shear
    zoom_range=0.2,  # Random zoom
    horizontal_flip=True,  # Random horizontal flip
    fill_mode='nearest'  # Fill mode
)

test_datagen = ImageDataGenerator(rescale=1./255)  # Only rescaling for test data

# Create data generators
train_generator = train_datagen.flow_from_directory(
    train_dir,  # Training directory
    target_size=(150, 150),  # Resize images to 150x150
    batch_size=32,  # Number of images to yield per batch
    class_mode='categorical'  # For multi-class classification
)

test_generator = test_datagen.flow_from_directory(
    test_dir,  # Test directory
    target_size=(150, 150),  # Resize images to 150x150
    batch_size=32,  # Number of images to yield per batch
    class_mode='categorical'  # For multi-class classification
)

# Display class indices
print(train_generator.class_indices)
print(test_generator.class_indices)

# Get a batch of images and labels
x_batch, y_batch = next(train_generator)

# Plot the images
plt.figure(figsize=(12, 12))
for i in range(9):
    plt.subplot(3, 3, i + 1)
    plt.imshow(x_batch[i])
    plt.axis('off')
plt.show()

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    MaxPooling2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(len(train_generator.class_indices), activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Print model summary
model.summary()

history = model.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    epochs=10,  # Number of epochs
    validation_data=test_generator,
    validation_steps=test_generator.samples // test_generator.batch_size
)

import matplotlib.pyplot as plt

def plot_history(history, title):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(1, len(acc) + 1)

    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, acc, 'bo-', label='Training accuracy')
    plt.plot(epochs, val_acc, 'ro-', label='Validation accuracy')
    plt.title(f'Training and validation accuracy - {title}')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(epochs, loss, 'bo-', label='Training loss')
    plt.plot(epochs, val_loss, 'ro-', label='Validation loss')
    plt.title(f'Training and validation loss - {title}')
    plt.legend()

    plt.show()

# Plot the history for the current model
plot_history(history, "Current Model")

"""**RESNET 50**"""

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, GlobalAveragePooling2D

base_model_resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(150, 150, 3))

model_resnet = Sequential([
    base_model_resnet,
    GlobalAveragePooling2D(),
    Dense(512, activation='relu'),
    Dense(len(train_generator.class_indices), activation='softmax')
])

base_model_resnet.trainable = False

model_resnet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history_resnet = model_resnet.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    epochs=5,
    validation_data=test_generator,
    validation_steps=test_generator.samples // test_generator.batch_size
)

model_resnet.summary()

"""**InceptionV3**"""

from tensorflow.keras.applications import InceptionV3
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, GlobalAveragePooling2D

base_model_inception = InceptionV3(weights='imagenet', include_top=False, input_shape=(150, 150, 3))

model_inception = Sequential([
    base_model_inception,
    GlobalAveragePooling2D(),
    Dense(512, activation='relu'),
    Dense(len(train_generator.class_indices), activation='softmax')
])

base_model_inception.trainable = False

model_inception.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history_inception = model_inception.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    epochs=5,
    validation_data=test_generator,
    validation_steps=test_generator.samples // test_generator.batch_size
)

"""**EfficientNetB0**"""

from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense, GlobalAveragePooling2D

base_model_efficientnet = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(150, 150, 3))

model_efficientnet = Sequential([
    base_model_efficientnet,
    GlobalAveragePooling2D(),
    Dense(512, activation='relu'),
    Dense(len(train_generator.class_indices), activation='softmax')
])

base_model_efficientnet.trainable = False

model_efficientnet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history_efficientnet = model_efficientnet.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    epochs=10,
    validation_data=test_generator,
    validation_steps=test_generator.samples // test_generator.batch_size
)

model_efficientnet.summary()

